{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for spark-jars folder\n",
    "SPARK_JARS_PATH = \"/path/to/spark-jars/*\"\n",
    "\n",
    "# JSON Parameters file\n",
    "PARAMS_PATH = \"params-db-Playlist.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "\n",
    "# Getting parameters\n",
    "with open(PARAMS_PATH, \"r\") as f:\n",
    "  params = json.load(f)\n",
    "\n",
    "print(\"\\nParameters:\\n\")\n",
    "print(yaml.dump(params, allow_unicode = True, default_flow_style = False, sort_keys = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"Generic Ingestion\") \\\n",
    "  .master(\"local[*]\") \\\n",
    "  .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "  .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.2\") \\\n",
    "  .config(\"spark.jars\", SPARK_JARS_PATH) \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading data\n",
    "print(\"\\nReading data ...\\n\")\n",
    "print(yaml.dump(params['source'], allow_unicode = True, default_flow_style = False, sort_keys = False))\n",
    "\n",
    "# Source [CSV]\n",
    "if params['source']['type'] == \"csv\":\n",
    "  print(\"\\nReading CSV file ...\")\n",
    "\n",
    "  df = spark.read.csv(\n",
    "      path = params['source']['path'],\n",
    "      sep = params['source']['separator'],\n",
    "      encoding = params['source']['encoding'],\n",
    "      header = params['source']['header'],\n",
    "      inferSchema = True\n",
    "    )\n",
    "\n",
    "# Source [Banco de Dados]\n",
    "elif params['source']['type'] == \"database\":\n",
    "  print(\"\\nReading database ...\")\n",
    "  QUERY = eval(f'f\"{\" \".join(params['source']['query']).strip()}\"')\n",
    "  print(QUERY)\n",
    "\n",
    "  df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"driver\", params['source']['jdbc_driver']) \\\n",
    "    .option(\"url\", params['source']['url']) \\\n",
    "    .option(\"dbtable\", f\"({QUERY})\") \\\n",
    "    .option(\"user\", params['source']['user']) \\\n",
    "    .option(\"password\", params['source']['password']) \\\n",
    "    .option(\"sessionInitStatement\", params['source']['execute_before_query']) \\\n",
    "    .load()\n",
    "\n",
    "# Source [API]\n",
    "elif params['source']['type'] == \"api\":\n",
    "  import requests\n",
    "  import pandas as pd\n",
    "  print(\"\\nReading api ...\")\n",
    "\n",
    "  response = requests.get(\n",
    "    params['source']['url'],\n",
    "    **params['source']['optional_params']\n",
    "  )\n",
    "\n",
    "  if params['source']['data_location_key']:\n",
    "    df_pd = pd.json_normalize(response.json()[params['source']['data_location_key']])\n",
    "  else:\n",
    "    df_pd = pd.json_normalize(response.json())\n",
    "\n",
    "  df = spark.createDataFrame(df_pd)\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df = df.withColumnsRenamed(params['data']['rename'])\n",
    "\n",
    "for col_name in df.columns:\n",
    "  df = df.withColumnRenamed(col_name, col_name.lower())\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim\n",
    "if params['data']['auto_trim']:\n",
    "  for column in df.columns:\n",
    "    if dict(df.dtypes)[column] == \"string\":\n",
    "      df = df.withColumn(column, trim(df[column]))\n",
    "\n",
    "# Pre treatment\n",
    "for treat_column_name, treat_column_value in params['data']['treat_columns'].items():\n",
    "  df = df.withColumn(treat_column_name, expr(treat_column_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast\n",
    "for col_cast in params['data']['schema']:\n",
    "  df = df \\\n",
    "    .withColumn(col_cast['col'], col(col_cast['col']).cast(col_cast['type'])) \\\n",
    "    .withMetadata(col_cast['col'], {'comment': col_cast['comment']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for py_column in params['data']['py_columns']:\n",
    "  df = df \\\n",
    "    .withColumn(py_column['col'], expr(py_column['value'])) \\\n",
    "    .withMetadata(py_column['col'], {'comment': py_column['comment']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in df.schema.fields:\n",
    "  print(f\"Coluna: {field.name}, Tipo: {field.dataType}, Comentário: {field.metadata.get('comment', 'Nenhum comentário')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save\n",
    "print(\"\\nSaving data ...\\n\")\n",
    "print(yaml.dump(params['destination'], allow_unicode = True, default_flow_style = False, sort_keys = False))\n",
    "\n",
    "catalog = params['destination']['catalog']\n",
    "warehouse = params['destination']['warehouse_path']\n",
    "\n",
    "spark.conf.set(f\"spark.sql.catalog.{catalog}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "spark.conf.set(f\"spark.sql.catalog.{catalog}.type\", \"hadoop\")\n",
    "spark.conf.set(f\"spark.sql.catalog.{catalog}.warehouse\", warehouse)\n",
    "\n",
    "df.write \\\n",
    "  .format(\"iceberg\") \\\n",
    "  .mode(params['destination'].get(\"mode\", \"overwrite\")) \\\n",
    "  .partitionBy(*params['destination'].get(\"partition_by\", [])) \\\n",
    "  .saveAsTable(f\"{params['destination']['catalog']}.{params['destination']['database']}.{params['destination']['table']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table(f\"{params['destination']['catalog']}.{params['destination']['database']}.{params['destination']['table']}\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".python-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
