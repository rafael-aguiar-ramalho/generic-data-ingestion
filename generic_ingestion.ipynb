{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for spark-jars folder\n",
    "SPARK_JARS_PATH = \"/path/to/spark-jars/*\"\n",
    "\n",
    "# JSON Parameters file\n",
    "PARAMS_PATH = \"params-db-Playlist.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "\n",
      "source:\n",
      "  type: database\n",
      "  jdbc_driver: org.sqlite.JDBC\n",
      "  url: jdbc:sqlite:data/Chinook_Sqlite.sqlite\n",
      "  query:\n",
      "  - SELECT *\n",
      "  - FROM Playlist\n",
      "  user: ''\n",
      "  password: ''\n",
      "  execute_before_query: 'SELECT 1 '\n",
      "data:\n",
      "  rename:\n",
      "    PlaylistId: playlist_id\n",
      "  auto_trim: true\n",
      "  treat_columns: {}\n",
      "  schema:\n",
      "  - col: playlist_id\n",
      "    type: integer\n",
      "    comment: Playlist ID\n",
      "  - col: name\n",
      "    type: string\n",
      "    comment: Playlist Name\n",
      "  py_columns:\n",
      "  - col: processed_at\n",
      "    value: current_timestamp()\n",
      "    comment: Date and time of data processing\n",
      "destination:\n",
      "  catalog: default\n",
      "  warehouse_path: file:///home/rafa-aguiar/Documentos/Projetos/data/write\n",
      "  database: raw\n",
      "  table: chinook_playlist\n",
      "  mode: overwrite\n",
      "  partition_by: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "\n",
    "# Getting parameters\n",
    "with open(PARAMS_PATH, \"r\") as f:\n",
    "  params = json.load(f)\n",
    "\n",
    "print(\"\\nParameters:\\n\")\n",
    "print(yaml.dump(params, allow_unicode = True, default_flow_style = False, sort_keys = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"Generic Ingestion\") \\\n",
    "  .master(\"local[*]\") \\\n",
    "  .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "  .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.2\") \\\n",
    "  .config(\"spark.jars\", SPARK_JARS_PATH) \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data ...\n",
      "\n",
      "type: database\n",
      "jdbc_driver: org.sqlite.JDBC\n",
      "url: jdbc:sqlite:data/Chinook_Sqlite.sqlite\n",
      "query:\n",
      "- SELECT *\n",
      "- FROM Playlist\n",
      "user: ''\n",
      "password: ''\n",
      "execute_before_query: 'SELECT 1 '\n",
      "\n",
      "\n",
      "Reading database ...\n",
      "SELECT * FROM Playlist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[PlaylistId: int, Name: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading data\n",
    "print(\"\\nReading data ...\\n\")\n",
    "print(yaml.dump(params['source'], allow_unicode = True, default_flow_style = False, sort_keys = False))\n",
    "\n",
    "# Source [CSV]\n",
    "if params['source']['type'] == \"csv\":\n",
    "  print(\"\\nReading CSV file ...\")\n",
    "\n",
    "  df = spark.read.csv(\n",
    "      path = params['source']['path'],\n",
    "      sep = params['source']['separator'],\n",
    "      encoding = params['source']['encoding'],\n",
    "      header = params['source']['header'],\n",
    "      inferSchema = True\n",
    "    )\n",
    "\n",
    "# Source [Banco de Dados]\n",
    "elif params['source']['type'] == \"database\":\n",
    "  print(\"\\nReading database ...\")\n",
    "  QUERY = eval(f'f\"{\" \".join(params['source']['query']).strip()}\"')\n",
    "  print(QUERY)\n",
    "\n",
    "  df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"driver\", params['source']['jdbc_driver']) \\\n",
    "    .option(\"url\", params['source']['url']) \\\n",
    "    .option(\"dbtable\", f\"({QUERY})\") \\\n",
    "    .option(\"user\", params['source']['user']) \\\n",
    "    .option(\"password\", params['source']['password']) \\\n",
    "    .option(\"sessionInitStatement\", params['source']['execute_before_query']) \\\n",
    "    .load()\n",
    "\n",
    "# Source [API]\n",
    "elif params['source']['type'] == \"api\":\n",
    "  import requests\n",
    "  import pandas as pd\n",
    "  print(\"\\nReading api ...\")\n",
    "\n",
    "  response = requests.get(\n",
    "    params['source']['url'],\n",
    "    **params['source']['optional_params']\n",
    "  )\n",
    "\n",
    "  if params['source']['data_location_key']:\n",
    "    df_pd = pd.json_normalize(response.json()[params['source']['data_location_key']])\n",
    "  else:\n",
    "    df_pd = pd.json_normalize(response.json())\n",
    "\n",
    "  df = spark.createDataFrame(df_pd)\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|PlaylistId|        Name|\n",
      "+----------+------------+\n",
      "|         1|       Music|\n",
      "|         2|      Movies|\n",
      "|         3|    TV Shows|\n",
      "|         4|  Audiobooks|\n",
      "|         5|  90’s Music|\n",
      "|         6|  Audiobooks|\n",
      "|         7|      Movies|\n",
      "|         8|       Music|\n",
      "|         9|Music Videos|\n",
      "|        10|    TV Shows|\n",
      "+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PlaylistId: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- playlist_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename columns\n",
    "df = df.withColumnsRenamed(params['data']['rename'])\n",
    "\n",
    "for col_name in df.columns:\n",
    "  df = df.withColumnRenamed(col_name, col_name.lower())\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim\n",
    "if params['data']['auto_trim']:\n",
    "  for column in df.columns:\n",
    "    if dict(df.dtypes)[column] == \"string\":\n",
    "      df = df.withColumn(column, trim(df[column]))\n",
    "\n",
    "# Pre treatment\n",
    "for treat_column_name, treat_column_value in params['data']['treat_columns'].items():\n",
    "  df = df.withColumn(treat_column_name, expr(treat_column_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast\n",
    "for col_cast in params['data']['schema']:\n",
    "  df = df \\\n",
    "    .withColumn(col_cast['col'], col(col_cast['col']).cast(col_cast['type'])) \\\n",
    "    .withMetadata(col_cast['col'], {'comment': col_cast['comment']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|playlist_id|        name|\n",
      "+-----------+------------+\n",
      "|          1|       Music|\n",
      "|          2|      Movies|\n",
      "|          3|    TV Shows|\n",
      "|          4|  Audiobooks|\n",
      "|          5|  90’s Music|\n",
      "|          6|  Audiobooks|\n",
      "|          7|      Movies|\n",
      "|          8|       Music|\n",
      "|          9|Music Videos|\n",
      "|         10|    TV Shows|\n",
      "+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for py_column in params['data']['py_columns']:\n",
    "  df = df \\\n",
    "    .withColumn(py_column['col'], expr(py_column['value'])) \\\n",
    "    .withMetadata(py_column['col'], {'comment': py_column['comment']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coluna: playlist_id, Tipo: IntegerType(), Comentário: Playlist ID\n",
      "Coluna: name, Tipo: StringType(), Comentário: Playlist Name\n",
      "Coluna: processed_at, Tipo: TimestampType(), Comentário: Date and time of data processing\n"
     ]
    }
   ],
   "source": [
    "for field in df.schema.fields:\n",
    "  print(f\"Coluna: {field.name}, Tipo: {field.dataType}, Comentário: {field.metadata.get('comment', 'Nenhum comentário')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data ...\n",
      "\n",
      "catalog: default\n",
      "warehouse_path: file:///home/rafa-aguiar/Documentos/Projetos/data/write\n",
      "database: raw\n",
      "table: chinook_playlist\n",
      "mode: overwrite\n",
      "partition_by: []\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 15:32:34 WARN HadoopTableOperations: Error reading version hint file file:/home/rafa-aguiar/Documentos/Projetos/data/write/raw/chinook_playlist/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: File file:/home/rafa-aguiar/Documentos/Projetos/data/write/raw/chinook_playlist/metadata/version-hint.text does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:357)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:354)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:303)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:121)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:645)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:575)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/20 15:32:34 WARN HadoopTableOperations: Error reading version hint file file:/home/rafa-aguiar/Documentos/Projetos/data/write/raw/chinook_playlist/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: File file:/home/rafa-aguiar/Documentos/Projetos/data/write/raw/chinook_playlist/metadata/version-hint.text does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.current(HadoopTableOperations.java:84)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:366)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:354)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:303)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:121)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:645)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:575)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "## Save\n",
    "print(\"\\nSaving data ...\\n\")\n",
    "print(yaml.dump(params['destination'], allow_unicode = True, default_flow_style = False, sort_keys = False))\n",
    "\n",
    "catalog = params['destination']['catalog']\n",
    "warehouse = params['destination']['warehouse_path']\n",
    "\n",
    "spark.conf.set(f\"spark.sql.catalog.{catalog}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "spark.conf.set(f\"spark.sql.catalog.{catalog}.type\", \"hadoop\")\n",
    "spark.conf.set(f\"spark.sql.catalog.{catalog}.warehouse\", warehouse)\n",
    "\n",
    "df.write \\\n",
    "  .format(\"iceberg\") \\\n",
    "  .mode(params['destination'].get(\"mode\", \"overwrite\")) \\\n",
    "  .partitionBy(*params['destination'].get(\"partition_by\", [])) \\\n",
    "  .saveAsTable(f\"{params['destination']['catalog']}.{params['destination']['database']}.{params['destination']['table']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|playlist_id|                name|        processed_at|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          1|               Music|2025-04-20 18:32:...|\n",
      "|          2|              Movies|2025-04-20 18:32:...|\n",
      "|          3|            TV Shows|2025-04-20 18:32:...|\n",
      "|          4|          Audiobooks|2025-04-20 18:32:...|\n",
      "|          5|          90’s Music|2025-04-20 18:32:...|\n",
      "|          6|          Audiobooks|2025-04-20 18:32:...|\n",
      "|          7|              Movies|2025-04-20 18:32:...|\n",
      "|          8|               Music|2025-04-20 18:32:...|\n",
      "|          9|        Music Videos|2025-04-20 18:32:...|\n",
      "|         10|            TV Shows|2025-04-20 18:32:...|\n",
      "|         11|     Brazilian Music|2025-04-20 18:32:...|\n",
      "|         12|           Classical|2025-04-20 18:32:...|\n",
      "|         13|Classical 101 - D...|2025-04-20 18:32:...|\n",
      "|         14|Classical 101 - N...|2025-04-20 18:32:...|\n",
      "|         15|Classical 101 - T...|2025-04-20 18:32:...|\n",
      "|         16|              Grunge|2025-04-20 18:32:...|\n",
      "|         17| Heavy Metal Classic|2025-04-20 18:32:...|\n",
      "|         18|         On-The-Go 1|2025-04-20 18:32:...|\n",
      "+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.table(f\"{params['destination']['catalog']}.{params['destination']['database']}.{params['destination']['table']}\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".python-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
